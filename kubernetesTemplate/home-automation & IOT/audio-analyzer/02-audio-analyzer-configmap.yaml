apiVersion: v1
kind: ConfigMap
metadata:
  name: audio-analyzer-script
  namespace: home-automation
data:
  test.py: |
    import numpy as np
    import time
    import json
    import paho.mqtt.client as mqtt
    import os
    import struct

    def analyze_real_audio():
        print("Starting real audio analyzer...")
        client = mqtt.Client(mqtt.CallbackAPIVersion.VERSION2)
        client.connect("10.104.150.172", 1883, 60)
        print("Connected to MQTT")
        
        # Try to find audio source
        audio_sources = [
            "/tmp/snapfifo/music_assistant",
            "/tmp/snapfifo"
        ]
        
        audio_source = None
        for source in audio_sources:
            if os.path.exists(source):
                print(f"Found audio source: {source}")
                audio_source = source
                break
        
        if not audio_source:
            print("No audio source found, using enhanced test data")
            generate_enhanced_test_data(client)
            return
        
        # Try real audio analysis
        try:
            energy_history = []
            beat_threshold = 1.3
            counter = 0
            
            if os.path.isdir(audio_source):
                # It's a directory, look for the fifo inside
                fifo_path = os.path.join(audio_source, "music_assistant")
                if not os.path.exists(fifo_path):
                    print(f"Creating fifo at {fifo_path}")
                    # Fall back to test data
                    generate_enhanced_test_data(client)
                    return
            else:
                fifo_path = audio_source
            
            print(f"Attempting to read from: {fifo_path}")
            with open(fifo_path, 'rb') as audio_stream:
                print("Audio stream opened successfully!")
                
                while True:
                    counter += 1
                    # Read audio chunk (1024 samples, 16-bit stereo)
                    chunk = audio_stream.read(4096)
                    if not chunk:
                        time.sleep(0.01)
                        continue
                    
                    # Convert to numpy array
                    try:
                        audio_data = np.frombuffer(chunk, dtype=np.int16)
                        if len(audio_data) < 512:
                            continue
                        
                        # Normalize audio
                        audio_float = audio_data.astype(np.float32) / 32768.0
                        
                        # FFT for frequency analysis
                        fft = np.fft.rfft(audio_float[:1024])
                        magnitude = np.abs(fft)
                        
                        # Frequency bands (more precise)
                        bass = np.mean(magnitude[1:20]) * 10000      # 20-400 Hz
                        mids = np.mean(magnitude[20:100]) * 10000    # 400-2000 Hz  
                        treble = np.mean(magnitude[100:200]) * 10000 # 2000-8000 Hz
                        
                        # Energy and beat detection
                        rms_energy = np.sqrt(np.mean(audio_float ** 2)) * 1000
                        energy_history.append(rms_energy)
                        if len(energy_history) > 20:
                            energy_history.pop(0)
                        
                        avg_energy = np.mean(energy_history) if energy_history else 0
                        beat = rms_energy > (avg_energy * beat_threshold) if avg_energy > 0 else False
                        
                        data = {
                            "bass": max(0, min(100, float(bass))),
                            "mids": max(0, min(100, float(mids))), 
                            "treble": max(0, min(100, float(treble))),
                            "beat": bool(beat),
                            "energy": max(0, min(100, float(rms_energy)))
                        }
                        
                        client.publish("audio/analysis", json.dumps(data))
                        if counter % 100 == 0:  # Print every 5 seconds
                            print(f"Real audio: bass={bass:.1f}, mids={mids:.1f}, treble={treble:.1f}, beat={beat}")
                        
                        time.sleep(0.05)  # 20 Hz for real-time response
                        
                    except Exception as e:
                        print(f"Audio processing error: {e}")
                        time.sleep(0.1)
                        
        except Exception as e:
            print(f"Failed to open audio stream: {e}")
            print("Falling back to enhanced test data")
            generate_enhanced_test_data(client)

    def generate_enhanced_test_data(client):
        print("Generating enhanced test data with music-like patterns...")
        counter = 0
        bass_phase = 0
        
        while True:
            counter += 1
            bass_phase += 0.1
            
            # Simulate music-like patterns
            bass = max(0, 50 + 30 * np.sin(bass_phase) + np.random.rand() * 20)
            mids = max(0, 40 + 20 * np.sin(bass_phase * 1.5) + np.random.rand() * 15)
            treble = max(0, 30 + 25 * np.sin(bass_phase * 2) + np.random.rand() * 10)
            
            # Simulate beat every ~2 seconds
            beat = (counter % 40) < 3  # Beat pattern
            energy = (bass + mids + treble) / 3
            
            data = {
                "bass": float(bass),
                "mids": float(mids), 
                "treble": float(treble),
                "beat": bool(beat),
                "energy": float(energy)
            }
            
            client.publish("audio/analysis", json.dumps(data))
            if counter % 40 == 0:  # Print every 2 seconds
                print(f"Enhanced test: bass={bass:.1f}, mids={mids:.1f}, treble={treble:.1f}, beat={beat}")
            
            time.sleep(0.05)  # 20 Hz

    if __name__ == "__main__":
        analyze_real_audio()
